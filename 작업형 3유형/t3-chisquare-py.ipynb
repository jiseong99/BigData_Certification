{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7088048,"sourceType":"datasetVersion","datasetId":1633303}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 카이제곱 적합도 검정\n고등학교에서는 졸업생들이 선택하는 대학 전공 분야의 선호도가 시간이 지남에 따라 변하지 않는다고 가정합니다. 학교 측은 최근 졸업생들의 전공 선택이 과거와 같은 패턴을 따르는지 알아보기 위해 적합도 검정을 실시하기로 결정했습니다.\n\n과거 자료에 따르면 졸업생들이 선택하는 전공의 분포는 다음과 같습니다:\n\n인문학: 20%\n사회과학: 30%\n자연과학: 25%\n공학: 15%\n기타: 10%\n올해 졸업한 학생 200명의 전공 선택 분포는 다음과 같았습니다:\n\n인문학: 30명\n사회과학: 60명\n자연과학: 50명\n공학: 40명\n기타: 20명\n이 데이터를 바탕으로, 졸업생들의 전공 선택 패턴이 과거와 유사한지를 알아보기 위해 카이제곱 적합도 검정을 실시해야 합니다. 유의 수준은 0.05로 설정합니다.\n\n1. 검정 통계량?\n2. p-value?\n3. 유의수준 하 귀무가설 기각 또는 채택?","metadata":{}},{"cell_type":"code","source":"from scipy import stats","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:35:06.253222Z","iopub.execute_input":"2023-11-30T19:35:06.253623Z","iopub.status.idle":"2023-11-30T19:35:06.790382Z","shell.execute_reply.started":"2023-11-30T19:35:06.253596Z","shell.execute_reply":"2023-11-30T19:35:06.789241Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import chisquare","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:36:33.754325Z","iopub.execute_input":"2023-11-30T19:36:33.754914Z","iopub.status.idle":"2023-11-30T19:36:33.760182Z","shell.execute_reply.started":"2023-11-30T19:36:33.754874Z","shell.execute_reply":"2023-11-30T19:36:33.758919Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#관측 빈도\nobserved = [30,60,50,40,20]\nexpected = [40,60,50,30,20]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:37:19.660741Z","iopub.execute_input":"2023-11-30T19:37:19.661214Z","iopub.status.idle":"2023-11-30T19:37:19.667041Z","shell.execute_reply.started":"2023-11-30T19:37:19.661181Z","shell.execute_reply":"2023-11-30T19:37:19.665891Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"chisquare(f_obs = observed, f_exp = expected)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:38:14.777399Z","iopub.execute_input":"2023-11-30T19:38:14.777830Z","iopub.status.idle":"2023-11-30T19:38:14.788563Z","shell.execute_reply.started":"2023-11-30T19:38:14.777796Z","shell.execute_reply":"2023-11-30T19:38:14.787684Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Power_divergenceResult(statistic=5.833333333333334, pvalue=0.21194558437271782)"},"metadata":{}}]},{"cell_type":"code","source":"help(chisquare)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:37:39.941245Z","iopub.execute_input":"2023-11-30T19:37:39.941628Z","iopub.status.idle":"2023-11-30T19:37:39.948162Z","shell.execute_reply.started":"2023-11-30T19:37:39.941597Z","shell.execute_reply":"2023-11-30T19:37:39.947168Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Help on function chisquare in module scipy.stats._stats_py:\n\nchisquare(f_obs, f_exp=None, ddof=0, axis=0)\n    Calculate a one-way chi-square test.\n    \n    The chi-square test tests the null hypothesis that the categorical data\n    has the given frequencies.\n    \n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category.  By default the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n        is the number of observed frequencies.  The default value of `ddof`\n        is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    \n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n    \n        chisq : float or ndarray\n            The chi-squared test statistic.  The value is a float if `axis` is\n            None or `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            return value `chisq` are scalars.\n    \n    See Also\n    --------\n    scipy.stats.power_divergence\n    scipy.stats.fisher_exact : Fisher exact test on a 2x2 contingency table.\n    scipy.stats.barnard_exact : An unconditional exact test. An alternative\n        to chi-squared test for small sample sizes.\n    \n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5. According to [3]_, the\n    total number of samples is recommended to be greater than 13,\n    otherwise exact tests (such as Barnard's Exact test) should be used\n    because they do not overreject.\n    \n    Also, the sum of the observed and expected frequencies must be the same\n    for the test to be valid; `chisquare` raises an error if the sums do not\n    agree within a relative tolerance of ``1e-8``.\n    \n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not chi-square, in which case this test\n    is not appropriate.\n    \n    References\n    ----------\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 8.\n           https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html\n    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n    .. [3] Pearson, Karl. \"On the criterion that a given system of deviations from the probable\n           in the case of a correlated system of variables is such that it can be reasonably\n           supposed to have arisen from random sampling\", Philosophical Magazine. Series 5. 50\n           (1900), pp. 157-175.\n    .. [4] Mannan, R. William and E. Charles. Meslow. \"Bird populations and\n           vegetation characteristics in managed and old-growth forests,\n           northeastern Oregon.\" Journal of Wildlife Management\n           48, 1219-1238, :doi:`10.2307/3801783`, 1984.\n    \n    Examples\n    --------\n    In [4]_, bird foraging behavior was investigated in an old-growth forest\n    of Oregon.\n    In the forest, 44% of the canopy volume was Douglas fir,\n    24% was ponderosa pine, 29% was grand fir, and 3% was western larch.\n    The authors observed the behavior of several species of birds, one of\n    which was the red-breasted nuthatch. They made 189 observations of this\n    species foraging, recording 43 (\"23%\") of observations in Douglas fir,\n    52 (\"28%\") in ponderosa pine, 54 (\"29%\") in grand fir, and 40 (\"21%\") in\n    western larch.\n    \n    Using a chi-square test, we can test the null hypothesis that the\n    proportions of foraging events are equal to the proportions of canopy\n    volume. The authors of the paper considered a p-value less than 1% to be\n    significant.\n    \n    Using the above proportions of canopy volume and observed events, we can\n    infer expected frequencies.\n    \n    >>> import numpy as np\n    >>> f_exp = np.array([44, 24, 29, 3]) / 100 * 189\n    \n    The observed frequencies of foraging were:\n    \n    >>> f_obs = np.array([43, 52, 54, 40])\n    \n    We can now compare the observed frequencies with the expected frequencies.\n    \n    >>> from scipy.stats import chisquare\n    >>> chisquare(f_obs=f_obs, f_exp=f_exp)\n    Power_divergenceResult(statistic=228.23515947653874, pvalue=3.3295585338846486e-49)\n    \n    The p-value is well below the chosen significance level. Hence, the\n    authors considered the difference to be significant and concluded\n    that the relative proportions of foraging events were not the same\n    as the relative proportions of tree canopy volume.\n    \n    Following are other generic examples to demonstrate how the other\n    parameters can be used.\n    \n    When just `f_obs` is given, it is assumed that the expected frequencies\n    are uniform and given by the mean of the observed frequencies.\n    \n    >>> chisquare([16, 18, 16, 14, 12, 12])\n    Power_divergenceResult(statistic=2.0, pvalue=0.84914503608460956)\n    \n    With `f_exp` the expected frequencies can be given.\n    \n    >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n    Power_divergenceResult(statistic=3.5, pvalue=0.62338762774958223)\n    \n    When `f_obs` is 2-D, by default the test is applied to each column.\n    \n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> chisquare(obs)\n    Power_divergenceResult(statistic=array([2.        , 6.66666667]), pvalue=array([0.84914504, 0.24663415]))\n    \n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n    \n    >>> chisquare(obs, axis=None)\n    Power_divergenceResult(statistic=23.31034482758621, pvalue=0.015975692534127565)\n    >>> chisquare(obs.ravel())\n    Power_divergenceResult(statistic=23.310344827586206, pvalue=0.01597569253412758)\n    \n    `ddof` is the change to make to the default degrees of freedom.\n    \n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n    Power_divergenceResult(statistic=2.0, pvalue=0.7357588823428847)\n    \n    The calculation of the p-values is done by broadcasting the\n    chi-squared statistic with `ddof`.\n    \n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n    Power_divergenceResult(statistic=2.0, pvalue=array([0.84914504, 0.73575888, 0.5724067 ]))\n    \n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we use ``axis=1``:\n    \n    >>> chisquare([16, 18, 16, 14, 12, 12],\n    ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n    ...           axis=1)\n    Power_divergenceResult(statistic=array([3.5 , 9.25]), pvalue=array([0.62338763, 0.09949846]))\n\n","output_type":"stream"}]},{"cell_type":"code","source":"stats.__all__","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:36:14.487548Z","iopub.execute_input":"2023-11-30T19:36:14.488686Z","iopub.status.idle":"2023-11-30T19:36:14.502593Z","shell.execute_reply.started":"2023-11-30T19:36:14.488647Z","shell.execute_reply":"2023-11-30T19:36:14.501311Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['BootstrapMethod',\n 'CensoredData',\n 'ConstantInputWarning',\n 'Covariance',\n 'DegenerateDataWarning',\n 'FitError',\n 'MonteCarloMethod',\n 'NearConstantInputWarning',\n 'PermutationMethod',\n 'alexandergovern',\n 'alpha',\n 'anderson',\n 'anderson_ksamp',\n 'anglit',\n 'ansari',\n 'arcsine',\n 'argus',\n 'barnard_exact',\n 'bartlett',\n 'bayes_mvs',\n 'bernoulli',\n 'beta',\n 'betabinom',\n 'betaprime',\n 'biasedurn',\n 'binned_statistic',\n 'binned_statistic_2d',\n 'binned_statistic_dd',\n 'binom',\n 'binom_test',\n 'binomtest',\n 'boltzmann',\n 'bootstrap',\n 'boschloo_exact',\n 'boxcox',\n 'boxcox_llf',\n 'boxcox_normmax',\n 'boxcox_normplot',\n 'bradford',\n 'brunnermunzel',\n 'burr',\n 'burr12',\n 'cauchy',\n 'chi',\n 'chi2',\n 'chi2_contingency',\n 'chisquare',\n 'circmean',\n 'circstd',\n 'circvar',\n 'combine_pvalues',\n 'contingency',\n 'cosine',\n 'cramervonmises',\n 'cramervonmises_2samp',\n 'crystalball',\n 'cumfreq',\n 'describe',\n 'dgamma',\n 'differential_entropy',\n 'directional_stats',\n 'dirichlet',\n 'dirichlet_multinomial',\n 'distributions',\n 'dlaplace',\n 'dunnett',\n 'dweibull',\n 'ecdf',\n 'energy_distance',\n 'entropy',\n 'epps_singleton_2samp',\n 'erlang',\n 'expectile',\n 'expon',\n 'exponnorm',\n 'exponpow',\n 'exponweib',\n 'f',\n 'f_oneway',\n 'false_discovery_control',\n 'fatiguelife',\n 'find_repeats',\n 'fisher_exact',\n 'fisk',\n 'fit',\n 'fligner',\n 'foldcauchy',\n 'foldnorm',\n 'friedmanchisquare',\n 'gamma',\n 'gausshyper',\n 'gaussian_kde',\n 'genexpon',\n 'genextreme',\n 'gengamma',\n 'genhalflogistic',\n 'genhyperbolic',\n 'geninvgauss',\n 'genlogistic',\n 'gennorm',\n 'genpareto',\n 'geom',\n 'gibrat',\n 'gmean',\n 'gompertz',\n 'goodness_of_fit',\n 'gstd',\n 'gumbel_l',\n 'gumbel_r',\n 'gzscore',\n 'halfcauchy',\n 'halfgennorm',\n 'halflogistic',\n 'halfnorm',\n 'hmean',\n 'hypergeom',\n 'hypsecant',\n 'invgamma',\n 'invgauss',\n 'invweibull',\n 'invwishart',\n 'iqr',\n 'jarque_bera',\n 'johnsonsb',\n 'johnsonsu',\n 'kappa3',\n 'kappa4',\n 'kde',\n 'kendalltau',\n 'kruskal',\n 'ks_1samp',\n 'ks_2samp',\n 'ksone',\n 'kstat',\n 'kstatvar',\n 'kstest',\n 'kstwo',\n 'kstwobign',\n 'kurtosis',\n 'kurtosistest',\n 'laplace',\n 'laplace_asymmetric',\n 'levene',\n 'levy',\n 'levy_l',\n 'levy_stable',\n 'linregress',\n 'loggamma',\n 'logistic',\n 'loglaplace',\n 'lognorm',\n 'logrank',\n 'logser',\n 'loguniform',\n 'lomax',\n 'mannwhitneyu',\n 'matrix_normal',\n 'maxwell',\n 'median_abs_deviation',\n 'median_test',\n 'mielke',\n 'mode',\n 'moment',\n 'monte_carlo_test',\n 'mood',\n 'morestats',\n 'moyal',\n 'mstats',\n 'mstats_basic',\n 'mstats_extras',\n 'multinomial',\n 'multiscale_graphcorr',\n 'multivariate_hypergeom',\n 'multivariate_normal',\n 'multivariate_t',\n 'mvn',\n 'mvsdist',\n 'nakagami',\n 'nbinom',\n 'ncf',\n 'nchypergeom_fisher',\n 'nchypergeom_wallenius',\n 'nct',\n 'ncx2',\n 'nhypergeom',\n 'norm',\n 'normaltest',\n 'norminvgauss',\n 'obrientransform',\n 'ortho_group',\n 'page_trend_test',\n 'pareto',\n 'pearson3',\n 'pearsonr',\n 'percentileofscore',\n 'permutation_test',\n 'planck',\n 'pmean',\n 'pointbiserialr',\n 'poisson',\n 'poisson_means_test',\n 'power_divergence',\n 'powerlaw',\n 'powerlognorm',\n 'powernorm',\n 'ppcc_max',\n 'ppcc_plot',\n 'probplot',\n 'qmc',\n 'randint',\n 'random_correlation',\n 'random_table',\n 'rankdata',\n 'ranksums',\n 'rayleigh',\n 'rdist',\n 'recipinvgauss',\n 'reciprocal',\n 'rel_breitwigner',\n 'relfreq',\n 'rice',\n 'rv_continuous',\n 'rv_discrete',\n 'rv_histogram',\n 'rvs_ratio_uniforms',\n 'scoreatpercentile',\n 'sem',\n 'semicircular',\n 'shapiro',\n 'siegelslopes',\n 'sigmaclip',\n 'skellam',\n 'skew',\n 'skewcauchy',\n 'skewnorm',\n 'skewtest',\n 'sobol_indices',\n 'somersd',\n 'spearmanr',\n 'special_ortho_group',\n 'statlib',\n 'stats',\n 'studentized_range',\n 't',\n 'theilslopes',\n 'tiecorrect',\n 'tmax',\n 'tmean',\n 'tmin',\n 'trapezoid',\n 'trapz',\n 'triang',\n 'trim1',\n 'trim_mean',\n 'trimboth',\n 'truncexpon',\n 'truncnorm',\n 'truncpareto',\n 'truncweibull_min',\n 'tsem',\n 'tstd',\n 'ttest_1samp',\n 'ttest_ind',\n 'ttest_ind_from_stats',\n 'ttest_rel',\n 'tukey_hsd',\n 'tukeylambda',\n 'tvar',\n 'uniform',\n 'uniform_direction',\n 'unitary_group',\n 'variation',\n 'vonmises',\n 'vonmises_fisher',\n 'vonmises_line',\n 'wald',\n 'wasserstein_distance',\n 'weibull_max',\n 'weibull_min',\n 'weightedtau',\n 'wilcoxon',\n 'wishart',\n 'wrapcauchy',\n 'yeojohnson',\n 'yeojohnson_llf',\n 'yeojohnson_normmax',\n 'yeojohnson_normplot',\n 'yulesimon',\n 'zipf',\n 'zipfian',\n 'zmap',\n 'zscore']"},"metadata":{}}]}]}